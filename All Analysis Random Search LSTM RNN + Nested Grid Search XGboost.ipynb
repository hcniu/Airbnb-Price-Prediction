{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "In this project, I try to predict the price of various Airbnbs, using **text feature, categorical features, and numeric features about the listings**. To be more specific, the text feature is the name and decription of the Airbnb. On the other hand, the categorical features include features, such as city of the listing and country of the listing. The numeric features include features, such as service fee.  \n",
    "  \n",
    "In order to utilize both text features and all the other features to come up with a prediction model, I plan to first **create a price prediction model using only the text features**. Then, I will **create another prediction model using the final prediction of the previous model as one of the feature and all the other features to predict the final price**. The final model will work similar as a **stacking model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "This is a public data provided by Airbnb. You can find the data source from the link below. Within the dataset, every single row represents a Airbnb listing. In the original data source, there are **26 columns and 102599 rows**.   \n",
    "  \n",
    "In the dataset, the **price** column will be the outcome and independent variable.\n",
    "  \n",
    "**Data Source: https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import data\n",
    "In this section, I import the raw data and rename the outcome variable as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/haochunniu/Desktop/Kaggle Compatition/Airbnb Open Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>NAME</th>\n",
       "      <th>host id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host name</th>\n",
       "      <th>neighbourhood group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>country</th>\n",
       "      <th>...</th>\n",
       "      <th>service fee</th>\n",
       "      <th>minimum nights</th>\n",
       "      <th>number of reviews</th>\n",
       "      <th>last review</th>\n",
       "      <th>reviews per month</th>\n",
       "      <th>review rate number</th>\n",
       "      <th>calculated host listings count</th>\n",
       "      <th>availability 365</th>\n",
       "      <th>house_rules</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001254</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>80014485718</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Madaline</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>$193</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10/19/2021</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>Clean up and treat the home the way you'd like...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002102</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>52335172823</td>\n",
       "      <td>verified</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>$28</td>\n",
       "      <td>30.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>5/21/2022</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Pet friendly but please confirm with me if the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002403</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>78829239556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elise</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Harlem</td>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>$124</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>I encourage you to use my kitchen, cooking and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                 NAME      host id  \\\n",
       "0  1001254   Clean & quiet apt home by the park  80014485718   \n",
       "1  1002102                Skylit Midtown Castle  52335172823   \n",
       "2  1002403  THE VILLAGE OF HARLEM....NEW YORK !  78829239556   \n",
       "\n",
       "  host_identity_verified host name neighbourhood group neighbourhood  \\\n",
       "0            unconfirmed  Madaline            Brooklyn    Kensington   \n",
       "1               verified     Jenna           Manhattan       Midtown   \n",
       "2                    NaN     Elise           Manhattan        Harlem   \n",
       "\n",
       "        lat      long        country  ... service fee minimum nights  \\\n",
       "0  40.64749 -73.97237  United States  ...       $193            10.0   \n",
       "1  40.75362 -73.98377  United States  ...        $28            30.0   \n",
       "2  40.80902 -73.94190  United States  ...       $124             3.0   \n",
       "\n",
       "  number of reviews last review  reviews per month review rate number  \\\n",
       "0               9.0  10/19/2021               0.21                4.0   \n",
       "1              45.0   5/21/2022               0.38                4.0   \n",
       "2               0.0         NaN                NaN                5.0   \n",
       "\n",
       "  calculated host listings count  availability 365  \\\n",
       "0                            6.0             286.0   \n",
       "1                            2.0             228.0   \n",
       "2                            1.0             352.0   \n",
       "\n",
       "                                         house_rules license  \n",
       "0  Clean up and treat the home the way you'd like...     NaN  \n",
       "1  Pet friendly but please confirm with me if the...     NaN  \n",
       "2  I encourage you to use my kitchen, cooking and...     NaN  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv('Airbnb_Open_Data.csv',header=0,low_memory=False)\n",
    "raw = raw.rename(columns={'price':'y'})\n",
    "raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data pre-processing and cleaning\n",
    "In this section, I carefully checked the data quality of the original dataset. To be more specific, I did the following inspections.  \n",
    "  \n",
    "**a.   Fix the price columns (y, service fee)**  \n",
    "**b.   Inspect and keep only the informative variables**  \n",
    "**c.   Keep records with no NA values in remaining columns**  \n",
    "**d.   Add the text of neighbourhood and room type into description**  \n",
    "**e.   Train, validation, and test data split**  \n",
    "**f .   Data vizualization on train data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_2596/3800995521.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  raw['y']=raw['y'].str.replace(\"$\",\"\")\n",
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_2596/3800995521.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  raw['service fee']=raw['service fee'].str.replace(\"$\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# a. Fix the price columns (y, service fee)\n",
    "# Originally all price were strings and started with a dollar sign. Thus, I would have to strip the dollar sign and turn into numeric data type\n",
    "raw['y']=raw['y'].replace([np.inf, -np.inf],np.nan)\n",
    "raw['y']=raw['y'].str.replace(\"$\",\"\")\n",
    "raw['y']=raw['y'].str.replace(\",\",\"\")\n",
    "raw['y']=raw['y'].astype(float)\n",
    "\n",
    "raw['service fee']=raw['service fee'].replace([np.inf, -np.inf],np.nan)\n",
    "raw['service fee']=raw['service fee'].str.replace(\"$\",\"\")\n",
    "raw['service fee']=raw['service fee'].str.replace(\",\",\"\")\n",
    "raw['service fee']=raw['service fee'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>describtion</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>room_type</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>966.0</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>False</td>\n",
       "      <td>strict</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142.0</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>verified</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>620.0</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>True</td>\n",
       "      <td>flexible</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y                          describtion host_identity_verified  \\\n",
       "0  966.0   Clean & quiet apt home by the park            unconfirmed   \n",
       "1  142.0                Skylit Midtown Castle               verified   \n",
       "2  620.0  THE VILLAGE OF HARLEM....NEW YORK !                    NaN   \n",
       "\n",
       "  neighbourhood_group instant_bookable cancellation_policy        room_type  \\\n",
       "0            Brooklyn            False              strict     Private room   \n",
       "1           Manhattan            False            moderate  Entire home/apt   \n",
       "2           Manhattan             True            flexible     Private room   \n",
       "\n",
       "   construction_year  \n",
       "0               2020  \n",
       "1               2007  \n",
       "2               2005  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Inspect and keep only the informative columns\n",
    "# In the end, I only kept 8 useful features\n",
    "raw['Construction year']=raw['Construction year'].astype('Int64')\n",
    "df1=raw[['y','NAME','host_identity_verified','neighbourhood group','instant_bookable','cancellation_policy','room type','Construction year']]\n",
    "df1=df1.rename(columns={'NAME':'describtion',\n",
    "                        'neighbourhood group':'neighbourhood_group',\n",
    "                        'room type':'room_type',\n",
    "                        'Construction year':'construction_year'})\n",
    "df1['neighbourhood_group']=np.where(df1['neighbourhood_group']=='brookln','Brooklyn',np.where(df1['neighbourhood_group']=='manhatan','Manhattan',df1['neighbourhood_group']))\n",
    "df1.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping the NAs, there are 102599 rows of data.\n",
      "----------------------------------------------------\n",
      "y                         0.24\n",
      "describtion               0.24\n",
      "host_identity_verified    0.28\n",
      "neighbourhood_group       0.03\n",
      "instant_bookable          0.10\n",
      "cancellation_policy       0.07\n",
      "room_type                 0.00\n",
      "construction_year         0.21\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# c. Keep records with no NA values in remaining columns\n",
    "# In the remaining columns, all columns had less than 0.3% of NA\n",
    "print('Before dropping the NAs, there are {} rows of data.'.format(len(df1)))\n",
    "print('----------------------------------------------------')\n",
    "print(round(df1.isna().sum()/len(df1)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping the NAs, there are 101544 rows of data.\n",
      "About 1.03% of rows are dropped.\n",
      "-----------------------------------------------------\n",
      "y                         0.0\n",
      "describtion               0.0\n",
      "host_identity_verified    0.0\n",
      "neighbourhood_group       0.0\n",
      "instant_bookable          0.0\n",
      "cancellation_policy       0.0\n",
      "room_type                 0.0\n",
      "construction_year         0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Now, there's no NA values within the data\n",
    "df2=df1.dropna()\n",
    "df2=df2.reset_index(drop=True)\n",
    "n=round((1-(len(df2)/len(df1)))*100,2)\n",
    "print('After dropping the NAs, there are {} rows of data.\\nAbout {}% of rows are dropped.'.format(len(df2),n))\n",
    "print('-----------------------------------------------------')\n",
    "print(round(df2.isna().sum()/len(df2)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Add the text of neighbourhood and room type into description\n",
    "# After sevral times of test and trial, I notice that if the desscribtion of the listing did not include the location and room type, \n",
    "# the first stage prediction model, the model that predict price with only the text describtion, will not have good performance. \n",
    "# Hence, I will add the text of neighbour hood and room type into the describtion \n",
    "x=[]\n",
    "for i in range(len(df2)):\n",
    "    tem=\"{}. This listing is a {} at {}.\".format(df2['describtion'][i],df2['room_type'][i],df2['neighbourhood_group'][i])\n",
    "    x.append(tem)\n",
    "df2['describtion']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 73111 rows of data in train data, 18278 rows of data in validation data, and 10155 rows of data in test data.\n"
     ]
    }
   ],
   "source": [
    "# e. Train, validation, and test data split\n",
    "# I used 72% of the data as train data, 18% of the data as validation data, and 10% as test data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(df2.drop(columns=['y']),\n",
    "                                                            df2['y'],\n",
    "                                                            test_size=0.1,\n",
    "                                                            random_state=9)\n",
    "train_val=pd.concat([x_train_val,y_train_val],axis=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_val.drop(columns=['y']),\n",
    "                                                  train_val['y'],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=99)\n",
    "\n",
    "print('There are totally {} rows of data in train data, {} rows of data in validation data, and {} rows of data in test data.'.format(len(y_train),len(y_val),len(y_test)))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reset_index(drop=True)\n",
    "x_val=x_val.reset_index(drop=True)\n",
    "x_test=x_test.reset_index(drop=True)\n",
    "y_train=y_train.reset_index(drop=True)\n",
    "y_val=y_val.reset_index(drop=True)\n",
    "y_test=y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f. Data visualization on train data\n",
    "# In this project, I use Tableau for visualization. In order to use Tableau, I output the full train data.\n",
    "# The link to the Tableau dashboard is https://public.tableau.com/views/AirbnbOpenData/Visualization?:language=en-US&publish=yes&:display_count=n&:origin=viz_share_link .\n",
    "# Feel free to view the dashboard!!!\n",
    "train_all=pd.concat([x_train,y_train],axis=1)\n",
    "train_all.to_csv('train_all.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model1: Price prediction model using listing describtion text  \n",
    "In this project, I would use the concept of stacking ensemble model. I will first create a price prediction model that will use only the describtion of the listing as feature.  \n",
    "To be more specific, I will try two different RNN model, simple RNN and LSTM, for the price prediction model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/vz0p8qdj36g5q9_w2fd7gcs00000gn/T/ipykernel_2596/1645776784.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. Extract only the text field\n",
    "train_text=np.array(x_train['describtion'])\n",
    "val_text=np.array(x_val['describtion'])\n",
    "test_text=np.array(x_test['describtion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-20 12:19:14.670841: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#2-1.Text pre-processing for train data\n",
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens = None,\n",
    "    standardize = 'lower_and_strip_punctuation',\n",
    "    split = 'whitespace',\n",
    "    ngrams = None,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After word embedding, there are 13238 words.\n"
     ]
    }
   ],
   "source": [
    "#2-2. Apply it to the text data with \"adapt\". \n",
    "vectorize_layer.adapt(train_text)\n",
    "print('After word embedding, there are {} words.'.format(len(vectorize_layer.get_vocabulary(10))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Simple RNN model  \n",
    "First, we try the most basic RNN model and use randomized search to find the best hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the RNN Random Search structure\n",
    "def build_model(hp):\n",
    "    model_rnn = keras.Sequential()\n",
    "\n",
    "    model_rnn.add(vectorize_layer)\n",
    "\n",
    "    model_rnn.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 256,\n",
    "    mask_zero = True\n",
    "    ))\n",
    "\n",
    "    model_rnn.add(keras.layers.SimpleRNN(units=hp.Int('units',\n",
    "                                                      min_value=10,\n",
    "                                                      max_value=400,\n",
    "                                                      step=10),\n",
    "                                         return_sequences=True,\n",
    "                                         dropout=0.2))\n",
    "    \n",
    "    model_rnn.add(keras.layers.SimpleRNN(units=hp.Int('units',\n",
    "                                                      min_value=10,\n",
    "                                                      max_value=400,\n",
    "                                                      step=10),\n",
    "                                         return_sequences=True,\n",
    "                                         dropout=0.2))\n",
    "\n",
    "    model_rnn.add(keras.layers.SimpleRNN(units=hp.Int('units',\n",
    "                                                      min_value=10,\n",
    "                                                      max_value=400,\n",
    "                                                      step=10),\n",
    "                                         dropout=0.2))\n",
    "\n",
    "    model_rnn.add(keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "    model_rnn.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',\n",
    "                                                                values=[0.01,0.001,0.0001])),\n",
    "                      loss='mse',\n",
    "                      metrics=['mse'])\n",
    "    return model_rnn\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective='mse',\n",
    "                   max_trials=2,\n",
    "                   overwrite=True, #Always remember to add this\n",
    "                   executions_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [01h 11m 46s]\n",
      "mse: 123975.3984375\n",
      "\n",
      "Best mse So Far: 109862.1484375\n",
      "Total elapsed time: 02h 27m 46s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Start the Search\n",
    "tuner.search(x=train_text,y=y_train,epochs=50,batch_size=256,validation_data=(val_text,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 3 layers RNN parameters would be 380 neurons and 0.01 learning rate.\n",
      "------------------------------------------\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x1594d0df0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 380\n",
      "learning_rate: 0.01\n",
      "Score: 109862.1484375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 360\n",
      "learning_rate: 0.0001\n",
      "Score: 123975.3984375\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 3 layers RNN parameters would be {} neurons and {} learning rate.'.format(result['units'],result['learning_rate']))\n",
    "print('------------------------------------------')\n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the final model\n",
    "from keras.models import load_model\n",
    "RNNmodel=tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280/2280 [==============================] - 254s 110ms/step - loss: 110297.9609 - mse: 110297.9609\n",
      "INFO:tensorflow:Assets written to: RNNmodel/assets\n"
     ]
    }
   ],
   "source": [
    "#Re-train and save the best model - Before saving the model, remember to re-train the model\n",
    "RNNmodel.fit(train_text,y_train)\n",
    "RNNmodel.save(\"RNNmodel\")\n",
    "#RNNmodel = load_model(\"RNNmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318/318 [==============================] - 7s 19ms/step\n",
      "572/572 [==============================] - 12s 21ms/step\n",
      "2285/2285 [==============================] - 50s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predict with the final model\n",
    "RNN_prediction_test=RNNmodel.predict(test_text)\n",
    "RNN_prediction_test=[i[0]for i in RNN_prediction_test]\n",
    "RNN_prediction_val=RNNmodel.predict(val_text)\n",
    "RNN_prediction_val=[i[0]for i in RNN_prediction_val]\n",
    "RNN_prediction_train=RNNmodel.predict(train_text)\n",
    "RNN_prediction_train=[i[0]for i in RNN_prediction_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of test data with the LSTM prediction model is 332.71\n"
     ]
    }
   ],
   "source": [
    "#RMSE on test data\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse=metrics.mean_squared_error(y_test,RNN_prediction_test)\n",
    "rmse=math.sqrt(mse)\n",
    "print('The RMSE of test data with the LSTM prediction model is {}'.format(round(rmse,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final prediction of the RNN model\n",
    "RNN_prediction_test=pd.DataFrame({'RNN_prediction_test':RNN_prediction_test})\n",
    "RNN_prediction_test.to_csv('RNN Prediction on Test data.csv')\n",
    "RNN_prediction_val=pd.DataFrame({'RNN_prediction_val':RNN_prediction_val})\n",
    "RNN_prediction_val.to_csv('RNN Prediction on Validation data.csv')\n",
    "RNN_prediction_train=pd.DataFrame({'RNN_prediction_train':RNN_prediction_train})\n",
    "RNN_prediction_train.to_csv('RNN Prediction on Train data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. LSTM model  \n",
    "Next, given that the performance of the simple RNN model is not as good as expected, I try the more advance method, the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the LSTM Random Search structure\n",
    "def build_model(hp):\n",
    "    model_lstm = keras.Sequential()\n",
    "\n",
    "    model_lstm.add(vectorize_layer)\n",
    "\n",
    "    model_lstm.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 256,\n",
    "    mask_zero = True\n",
    "    ))\n",
    "\n",
    "    model_lstm.add(keras.layers.LSTM(units=hp.Int('units',\n",
    "                                                  min_value=10,\n",
    "                                                  max_value=400,\n",
    "                                                  step=10),\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=0.2))\n",
    "    \n",
    "    model_lstm.add(keras.layers.LSTM(units=hp.Int('units',\n",
    "                                                  min_value=10,\n",
    "                                                  max_value=400,\n",
    "                                                  step=10),\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=0.2))\n",
    "\n",
    "    model_lstm.add(keras.layers.LSTM(units=hp.Int('units',\n",
    "                                                  min_value=10,\n",
    "                                                  max_value=400,\n",
    "                                                  step=10),\n",
    "                                     dropout=0.2))\n",
    "\n",
    "    model_lstm.add(keras.layers.Dense(1,activation='linear'))\n",
    "\n",
    "    model_lstm.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',\n",
    "                                                                values=[0.01,0.001,0.0001])),\n",
    "                       loss='mse',\n",
    "                       metrics=['mse'])\n",
    "    return model_lstm\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective='mse',\n",
    "                   max_trials=2,\n",
    "                   overwrite=True, #Always remember to add this\n",
    "                   executions_per_trial=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [01h 40m 25s]\n",
      "mse: 207848.34375\n",
      "\n",
      "Best mse So Far: 108563.9375\n",
      "Total elapsed time: 03h 41m 22s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Start the Search\n",
    "tuner.search(x=train_text,y=y_train,epochs=50,batch_size=256,validation_data=(val_text,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 3 layers LSTM parameters would be 260 neurons and 0.01 learning rate.\n",
      "------------------------------------------\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x15966d0f0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 260\n",
      "learning_rate: 0.01\n",
      "Score: 108563.9375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 220\n",
      "learning_rate: 0.0001\n",
      "Score: 207848.34375\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 3 layers LSTM parameters would be {} neurons and {} learning rate.'.format(result['units'],result['learning_rate']))\n",
    "print('------------------------------------------')\n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2280/2280 [==============================] - 345s 147ms/step - loss: 109223.3125 - mse: 109223.3125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTMmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: LSTMmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 15s 36ms/step\n",
      "570/570 [==============================] - 21s 37ms/step\n",
      "2280/2280 [==============================] - 85s 37ms/step\n"
     ]
    }
   ],
   "source": [
    "#Get the final model\n",
    "from keras.models import load_model\n",
    "LSTMmodel=tuner.get_best_models()[0]\n",
    "\n",
    "#Re-train and save the best model - Before saving the model, remember to re-train the model\n",
    "LSTMmodel.fit(train_text,y_train)\n",
    "LSTMmodel.save(\"LSTMmodel\")\n",
    "#LSTMmodel = load_model(\"LSTMmodel\") Load Model\n",
    "#LSTMmodel.predict(...) Use the model, it is already trained\n",
    "\n",
    "#Predict with the final model\n",
    "LSTM_prediction_test=LSTMmodel.predict(test_text)\n",
    "LSTM_prediction_test=[i[0]for i in LSTM_prediction_test]\n",
    "LSTM_prediction_val=LSTMmodel.predict(val_text)\n",
    "LSTM_prediction_val=[i[0]for i in LSTM_prediction_val]\n",
    "LSTM_prediction_train=LSTMmodel.predict(train_text)\n",
    "LSTM_prediction_train=[i[0]for i in LSTM_prediction_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of test data with the LSTM prediction model is 331.23\n"
     ]
    }
   ],
   "source": [
    "#RMSE on test data\n",
    "from sklearn import metrics\n",
    "import math\n",
    "mse=metrics.mean_squared_error(y_test,LSTM_prediction_test)\n",
    "rmse=math.sqrt(mse)\n",
    "print('The RMSE of test data with the LSTM prediction model is {}'.format(round(rmse,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final prediction of the RNN model\n",
    "LSTM_prediction_test=pd.DataFrame({'LSTM_prediction_test':LSTM_prediction_test})\n",
    "LSTM_prediction_test.to_csv('LSTM Prediction on Test data.csv')\n",
    "LSTM_prediction_val=pd.DataFrame({'LSTM_prediction_val':LSTM_prediction_val})\n",
    "LSTM_prediction_val.to_csv('LSTM Prediction on Validation data.csv')\n",
    "LSTM_prediction_train=pd.DataFrame({'LSTM_prediction_train':LSTM_prediction_train})\n",
    "LSTM_prediction_train.to_csv('LSTM Prediction on Train data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model2: Stacking price prediction model using the prediction from the previous model and the rest of the features\n",
    "After finishing the first stage model, I will build a stacking model. The model will use the prediction of the previous model and the rest of the features as input. Here, we will use nested random search CV to help me find the best performing model and use random search to find the best hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.model_selection import cross_val_score,KFold,RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317/317 [==============================] - 14s 35ms/step\n",
      "570/570 [==============================] - 21s 36ms/step\n",
      "2280/2280 [==============================] - 86s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predict with the final model\n",
    "LSTM_prediction_test=LSTMmodel.predict(test_text)\n",
    "LSTM_prediction_test=[i[0]for i in LSTM_prediction_test]\n",
    "LSTM_prediction_val=LSTMmodel.predict(val_text)\n",
    "LSTM_prediction_val=[i[0]for i in LSTM_prediction_val]\n",
    "LSTM_prediction_train=LSTMmodel.predict(train_text)\n",
    "LSTM_prediction_train=[i[0]for i in LSTM_prediction_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['first_stage_pred']=LSTM_prediction_train\n",
    "x_val['first_stage_pred']=LSTM_prediction_val\n",
    "x_test['first_stage_pred']=LSTM_prediction_test\n",
    "x_train=x_train.drop(columns=['describtion'])\n",
    "x_val=x_val.drop(columns=['describtion'])\n",
    "x_test=x_test.drop(columns=['describtion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all the categorical variables to dummy variables\n",
    "x_train['instant_bookable']=np.where(x_train['instant_bookable'],1,0)\n",
    "x_train['instant_bookable']=x_train['instant_bookable'].astype('uint8')\n",
    "x_train['construction_year']=x_train['construction_year'].astype(int)\n",
    "x_train=pd.get_dummies(x_train,columns=['host_identity_verified','neighbourhood_group','cancellation_policy','room_type'])\n",
    "x_val['instant_bookable']=np.where(x_val['instant_bookable'],1,0)\n",
    "x_val['instant_bookable']=x_val['instant_bookable'].astype('uint8')\n",
    "x_val['construction_year']=x_val['construction_year'].astype(int)\n",
    "x_val=pd.get_dummies(x_val,columns=['host_identity_verified','neighbourhood_group','cancellation_policy','room_type'])\n",
    "x_test['instant_bookable']=np.where(x_test['instant_bookable'],1,0)\n",
    "x_test['instant_bookable']=x_test['instant_bookable'].astype('uint8')\n",
    "x_test['construction_year']=x_test['construction_year'].astype(int)\n",
    "x_test=pd.get_dummies(x_test,columns=['host_identity_verified','neighbourhood_group','cancellation_policy','room_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the second model, because I use cross-validation, I don't need to seperate the validation data out anymore. I need to merge the train and validation data.\n",
    "x_train_val=pd.concat([x_train,x_val])\n",
    "x_train_val=x_train_val.reset_index(drop=True)\n",
    "y_train_val=pd.concat([y_train,y_val])\n",
    "y_train_val=y_train_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Nested random search to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Classifier\n",
    "rf=RandomForestRegressor(random_state=9)\n",
    "xgb=XGBRegressor(seed=9,objective='reg:squarederror',use_label_encoder =False,verbosity = 0)\n",
    "lgbm=lgb.LGBMRegressor(objective='regression',random_state=9)\n",
    "\n",
    "##############################################################\n",
    "# 2. Create the parameter grid\n",
    "rf_grid={'n_estimators':list(range(100,1100,100)),\n",
    "         'max_depth':list(range(3,11))}\n",
    "xgb_grid={'eta':np.arange(0.1,0.6,0.1),\n",
    "          'max_depth':list(range(3,16)),\n",
    "          'n_estimators':list(range(10,310,10)),\n",
    "          'gamma':list(range(1,6))}\n",
    "lgbm_grid={'learning_rate':np.arange(0.1,0.6,0.1),\n",
    "           'max_depth':list(range(3,16)),\n",
    "           'n_estimators':list(range(10,310,10))}\n",
    "\n",
    "##############################################################\n",
    "# 3. Create the CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "outer_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "\n",
    "##############################################################\n",
    "# 4-1-1. Random-search CV for Random Forest\n",
    "clf = RandomizedSearchCV(rf,rf_grid,cv=inner_cv,scoring='neg_root_mean_squared_error',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-1-2. Nested CV for Random Forest\n",
    "nested_score = cross_val_score(clf,X=x_train_val, y=y_train_val, cv=outer_cv,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# 4-1-3. Result for Nested CV\n",
    "rf_result=nested_score.mean()\n",
    "\n",
    "##############################################################\n",
    "# 4-2-1. Random-search CV for XGBoost Classifier\n",
    "clf = RandomizedSearchCV(xgb,xgb_grid,cv=inner_cv,scoring='neg_root_mean_squared_error',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-2-2. Nested CV for XGBoost Classifier\n",
    "nested_score = cross_val_score(clf,X=x_train_val, y=y_train_val, cv=outer_cv,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# 4-2-3. Result for Nested CV\n",
    "xgb_result=nested_score.mean()\n",
    "\n",
    "##############################################################\n",
    "# 4-3-1. Random-search CV for LightGBM Classifier\n",
    "clf = RandomizedSearchCV(lgbm,lgbm_grid,cv=inner_cv,scoring='neg_root_mean_squared_error',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-3-2. Nested CV for LightGBM Classifier\n",
    "nested_score = cross_val_score(clf,X=x_train_val, y=y_train_val, cv=outer_cv,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# 4-3-3. Result for Nested CV\n",
    "lgbm_result=nested_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE of Random Forest Classifier: 328.85\n",
      "Average RMSE of XGBoost Classifier: 323.04\n",
      "Average RMSE of LightGBM Classifier: 330.2\n"
     ]
    }
   ],
   "source": [
    "print('Average RMSE of Random Forest Classifier: {}'.format(round(-1*rf_result,2)))\n",
    "print('Average RMSE of XGBoost Classifier: {}'.format(round(-1*xgb_result,2)))\n",
    "print('Average RMSE of LightGBM Classifier: {}'.format(round(-1*lgbm_result,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Use Random search to find the best hyper-parameter for the XGBoost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Classifier\n",
    "xgb=XGBRegressor(seed=9,objective='reg:squarederror',use_label_encoder =False,verbosity = 0)\n",
    "\n",
    "# 2. Create the parameter grid\n",
    "xgb_grid={'eta':np.arange(0.1,0.6,0.1),\n",
    "          'max_depth':list(range(3,16)),\n",
    "          'n_estimators':list(range(10,310,10)),\n",
    "          'gamma':list(range(1,6))}\n",
    "\n",
    "# 3. Create the CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "\n",
    "# 4. Grid-search\n",
    "xgbmodel = RandomizedSearchCV(xgb,xgb_grid,cv=5,scoring='neg_root_mean_squared_error',n_iter=15,random_state=9)\n",
    "\n",
    "# 4. Fit the model\n",
    "xgbmodel.fit(x_train_val,y_train_val)\n",
    "\n",
    "# 5. Predict\n",
    "y_pred=xgbmodel.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With CV random search, I found the best hyperparameter is eta (learning rate) =0.2, max_depth=13, gamma=5, and n_estimators=250.\n",
      "Prediction MSE on Test Data: 92919.64\n"
     ]
    }
   ],
   "source": [
    "# 6. Result\n",
    "print (\"With CV random search, I found the best hyperparameter is eta (learning rate) ={}, max_depth={}, gamma={}, and n_estimators={}.\".format(xgbmodel.best_params_['eta'],\n",
    "                                                                                                                                         xgbmodel.best_params_['max_depth'],\n",
    "                                                                                                                                         xgbmodel.best_params_['gamma'],\n",
    "                                                                                                                                         xgbmodel.best_params_['n_estimators'],))\n",
    "print(\"Prediction MSE on Test Data: {}\".format(round(metrics.mean_squared_error(y_test,y_pred),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_XGB_model.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Save the final model\n",
    "import joblib\n",
    "joblib.dump(xgbmodel.best_estimator_, 'final_XGB_model.pkl') #Save the model\n",
    "#XGBmodel = joblib.load('final_XGB_model.pkl') Load the model\n",
    "#XGBmodel.predict(...) Use the model, the model is already trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "By inspecting the performance of the model on test data, I noticed that the model is not performing as good as expected. The mean absolut percentage error is around 40%. Yet, I believe the main cause is not the model itself or the techniques. Via visualization, it is clear that most of the features do not have predictive ability. Further more, even by taking the text feaure into account, there isn't too much improvement in model's performance. Hence, for further analysis and improve the prediction model, I would recommend to collect some more effective features first before focusing more on the modeling part."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
